# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uqtdvmcdq-HjTFj6-KAk6zNZSOX92h9O
"""

from transformers import Trainer, DataCollatorForLanguageModeling
import wandb


class GPT2TrainerHF:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def train(self, train_dataset, eval_dataset, training_args):
        # Créer un Data Collator pour le masquage dynamique
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False  # Pas de masquage aléatoire pour GPT-2
        )

        # Créer le Trainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
        )

        # Lancer l'entraînement
        trainer.train()

        # Terminer la session W&B
        wandb.finish()