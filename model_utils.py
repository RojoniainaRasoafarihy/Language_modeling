# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uqtdvmcdq-HjTFj6-KAk6zNZSOX92h9O
"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch


def load_model_and_tokenizer(model_name="gpt2"):
    # Charger le tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)

    # Ajouter un token de padding si absent
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Charger le mod√®le
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)

    return model, tokenizer